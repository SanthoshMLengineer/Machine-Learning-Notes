{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPjfCgOnnVNMKcwkeiCCEB+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**<h1><center>OVERFITTING AND UNDERFITTING</center></h1>**\n","\n","Before understanding overfitting and underfitting let discuss Bias and Varience.\n","\n","<u>Bias</u> :  \n","It is error between actual value of training data and predicted value of training data. If the average predicted values are far off from the actual values then the bias is high. \n","\n","If model has high Bias then the  model is too simple and it does not capture any underlying pattern of data.\n","\n","<u>Varience</u> : \n","\n","It is error between actual value of testing data and predicted value of testing data. High varience causes overfitting.\n","\n","**<h3>Underfitting :</h3>**\n","\n","If a model does not capture underlying pattern of data then their is high Bias this causes model Underfitting.  \n","\n","` Underfitting = High bias`\n","\n","<u>Handling Underfitting:</u>\n","\n","1. Get more training data.\n","2. Increase the size or number of parameters in the model.\n","3. Increase the complexity of the model.\n","4. Increasing the training time, until cost function is minimised.\n","\n","**<h3>Overfitting:</h3>**\n","\n","In data, you can think of `signal` is true underlying pattern and `noise` is some irrelevent information in data. If the algorithm is too complex , it can end up memorizing the noise instead of finding the signal.\n","\n","This overfit model will then make predictions based on that noise. It will perform unusually well on its training data but very poorly on new, unseen data.\n","\n","`Overfitting = Low bias + High variance.`\n","\n","<u>Handling Overfitting:</u>\n","\n","1. Removing Irrelevent input features\n","\n","2. Removing outliers or anamolies\n","3. Regularization : This is a form of regression, that regularizes or shrinks the coefficient estimates towards zero. This technique discourages learning a more complex model.\n","4. Early stopping : We can stop the training when the accuracy is reached mimimum, this prevents the model from memorizing the noise. \n","5. Dropout : This is a technique where randomly selected neurons are ignored during training.\n","6. Regularize the weights.\n","\n","\n"],"metadata":{"id":"VjIud15WvAHu"}}]}