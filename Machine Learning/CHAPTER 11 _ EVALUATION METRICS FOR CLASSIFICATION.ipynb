{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNb77Md+CGE6iGEH5cpXZ6b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**<h1><center>EVALUATION METRICS FOR CLASSIFICATION</center></h1>**\n","\n","After building a classification model and getting some output in forms of a probability or a class, the next step is to find out how effective is the model based on some metric using test datasets. \n","\n","For Classification Problems Different Evaluation metrics are used:\n","- Confusion Matrix\n","\n","- Accuracy\n","\n","- Precision\n","\n","- Recall or Sensitivity\n","\n","- Specificity\n","\n","- F1 Score\n","\n","**<u>1. Confusion Matrix</u>**\n","\n","This metric is most intuitive and helps to see model performance. It is used for classification problem. \n","\n","Let’s say we are solving a classification problem where we are predicting whether a person is having cancer or not. Let’s give a label of to our target variable:\n","- 1: When a person is having cancer \n","\n","- 0: When a person is NOT having cancer.\n","\n","<center><img src=\"https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png\"></center>\n","\n","<u>Few Terminology used in confusion matrix : </u>\n","\n","**True Positives (TP):**\n","\n","True positives are the cases when the actual class of the data point is 1(True) and the predicted is also 1(True)\n","     \n","Ex: The case where a person is actually having cancer(1) and the model classifying his case as cancer(1) comes under True positive.\n","    \n","**True Negatives (TN):**\n","\n","True negatives are the cases when the actual class of the data point is 0(False) and the predicted is also 0(False\n","\n","Ex: The case where a person NOT having cancer and the model classifying his case as Not cancer comes under True Negatives.\n","\n","**False Positives (FP):**\n","False positives are the cases when the actual class of the data point is 0(False) and the predicted is 1(True). False is because the model has predicted incorrectly and positive because the class predicted was a positive one. (1)\n","\n","Ex: A person NOT having cancer and the model classifying his case as cancer comes under False Positives.\n","\n","**False Negatives (FN):**\n","\n","False negatives are the cases when the actual class of the data point was 1(True) and the predicted is 0(False). False is because the model has predicted incorrectly and negative because the class predicted was a negative one. (0)\n","\n","Ex: A person having cancer and the model classifying his case as No-cancer comes under False Negatives.\n","\n","We can use Skearn Package to find the confusion_matrix , where y_true is actual value and y_pred is output from the model.\n","        \n","```\n","from sklearn.metrics import confusion_matrix\n","\n","confusion_matrix(y_true, y_pred)\n","```\n","\n","\n","**When to minimise what?**\n","\n","There’s no hard rule that says what should be minimised in all the situations. It purely depends on the business needs and the context of the problem you are trying to solve. Based on that, we might want to minimise either False Positives or False negatives.\n","\n","1. <u>Minimising False Negatives:</u>\n","\n","  Let's take one example cancer detection problem example, `1: cancer and 0 : Non-cancerous`. If the model predicted non-cancerous patient as cancerous then it is okey as it is less dangerous. If the model predictes cancerous patient as non-cancerous patient then this will be more dangerous. In this case we need to minimise False Negatives.\n","\n","\n","2. <u>Minimising False Positives:</u>\n","  \n","  For better understanding of False Positives, let’s use a different example where the model classifies whether an email is spam or not\n","  \n","  Let’s say that you are expecting an important email like hearing back from a recruiter or awaiting an admit letter from a university. Let’s assign a label to the target variable and say,1: “Email is a spam” and 0:”Email is not a spam”\n","  \n","  Suppose model classifies important mail is classified as `Spam`. In this scenario we need to minimise False Positive is more important than False Negatives. "],"metadata":{"id":"omDziZ5oDz9O"}},{"cell_type":"markdown","source":["**<u>2. Accuracy:</u>**\n","\n","Accuracy in classification problems is the number of correct predictions made by the model over all kinds predictions made.\n","<img src=\"https://www.researchgate.net/publication/336402347/figure/fig3/AS:812472659349505@1570719985505/Calculation-of-Precision-Recall-and-Accuracy-in-the-confusion-matrix.ppm\">\n","\n","\n","It can be calculated correct prediction given by algorithm divided by all kind of prediction given by algorithm (correct and wrong ones).\n","\n","**When to use Accuracy:**\n","\n","- Accuracy is a good measure when the target variable classes in the data are nearly balanced.\n","- Ex:60% classes in our fruits images data are apple and 40% are oranges. A model which predicts whether a new image is Apple or an Orange, 97% of times correctly is a very good measure in this example.\n","\n","**When NOT to use Accuracy:**\n","\n","- Accuracy should NEVER be used as a measure when the target variable classes in the data are a majority of one class.\n","- Ex: In our cancer detection example with 100 people, only 5 people has cancer. Let’s say our model is very bad and predicts every case as No Cancer. In doing so, it has classified those 95 non-cancer patients correctly and 5 cancerous patients as Non-cancerous. Now even though the model is terrible at predicting cancer, The accuracy of such a bad model is also 95%.\n","\n","We can use Skearn Package to find the accuracy_score , where y_true is actual value and y_pred is output from the model.\n","\n","```\n","from sklearn.metrics import accuracy_score\n","\n","accuracy_score(y_true, y_pred)\n","```"],"metadata":{"id":"psaNEY4MIDtc"}},{"cell_type":"markdown","source":["**<u>3. Precision</u>**\n","\n","It is calculated True Positive divided by all values that is  predicted as positive. This can be used when we want to minimise False Positive. \n","\n","<center><img src = \"https://miro.medium.com/max/875/1*yDuAOkdGvY4MC2Jp4VUl5Q.png\" width = 400px></center>\n","\n","\n","**<u>4. Recall or Sensitivity</u>**\n","\n","It is calculated True Positive divided by Actual Positive. This can be used when we want to minimise False Negative.\n","\n","<img src=\"https://www.researchgate.net/publication/336402347/figure/fig3/AS:812472659349505@1570719985505/Calculation-of-Precision-Recall-and-Accuracy-in-the-confusion-matrix.ppm\">\n","\n","<center><img src=\"https://miro.medium.com/max/875/1*8tyIfvFNXLE1egsfDj62QA.png\" width = 400px></center>\n","\n","**<u>5. Specificity</u>**\n","\n","It is calculated True Negative divided by all values that is predicted as Negatives. \n","\n","`Specificity =  TRUE NEGATIVE / TRUE NEGATIVE + FALSE POSITIVE`\n","\n","**<u>6. F1-Score</u>**\n","\n","F1-Score is a harmonic mean of precision and recall. It can be used when we want both precision and recall should be maximize. \n","\n","Let us take one example precision is 1.0 and recall is 0.0 then arithmetic mean is 0.5 that is not true because recall rate  is 0.0, but F1-score will give 0. It gives equal value for both precision and recall.\n","\n","We can use Skearn Package to find the f1_score , where y_true is actual value and y_pred is output from the model.\n","\n","```\n","from sklearn.metrics import f1_score\n","\n","f1_score(y_true, y_pred, average='macro')\n","```"],"metadata":{"id":"VvKaJwzcu3NE"}}]}