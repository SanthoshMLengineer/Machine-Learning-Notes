{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNMBC2XrHvT8Cv9nkLPA9Of"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"id":"3PVMDDywUBxQ","outputId":"c7f81831-1299-4c65-9497-9c13b9c0bf5f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667310840359,"user_tz":-330,"elapsed":7957,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n"]}],"source":["!pip install xgboost"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"MLMNYenZUBxU","executionInfo":{"status":"ok","timestamp":1667310840360,"user_tz":-330,"elapsed":25,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[],"source":["# Importing the required Package\n","import pandas as pd\n","\n","from xgboost import XGBClassifier\n","\n","from sklearn import datasets\n","\n","from sklearn.metrics import accuracy_score,confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import roc_auc_score\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","import statistics\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"rFZMHeONUBxZ","outputId":"af13abf3-6b5f-4d42-d9d7-c89bff40bfde","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667310840361,"user_tz":-330,"elapsed":24,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":[".. _iris_dataset:\n","\n","Iris plants dataset\n","--------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 150 (50 in each of three classes)\n","    :Number of Attributes: 4 numeric, predictive attributes and the class\n","    :Attribute Information:\n","        - sepal length in cm\n","        - sepal width in cm\n","        - petal length in cm\n","        - petal width in cm\n","        - class:\n","                - Iris-Setosa\n","                - Iris-Versicolour\n","                - Iris-Virginica\n","                \n","    :Summary Statistics:\n","\n","    ============== ==== ==== ======= ===== ====================\n","                    Min  Max   Mean    SD   Class Correlation\n","    ============== ==== ==== ======= ===== ====================\n","    sepal length:   4.3  7.9   5.84   0.83    0.7826\n","    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n","    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n","    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n","    ============== ==== ==== ======= ===== ====================\n","\n","    :Missing Attribute Values: None\n","    :Class Distribution: 33.3% for each of 3 classes.\n","    :Creator: R.A. Fisher\n","    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n","    :Date: July, 1988\n","\n","The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n","from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n","Machine Learning Repository, which has two wrong data points.\n","\n","This is perhaps the best known database to be found in the\n","pattern recognition literature.  Fisher's paper is a classic in the field and\n","is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n","data set contains 3 classes of 50 instances each, where each class refers to a\n","type of iris plant.  One class is linearly separable from the other 2; the\n","latter are NOT linearly separable from each other.\n","\n",".. topic:: References\n","\n","   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n","     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n","     Mathematical Statistics\" (John Wiley, NY, 1950).\n","   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n","     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n","   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n","     Structure and Classification Rule for Recognition in Partially Exposed\n","     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n","     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n","   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n","     on Information Theory, May 1972, 431-433.\n","   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n","     conceptual clustering system finds 3 classes in the data.\n","   - Many, many more ...\n"]}],"source":["# Load data\n","iris = datasets.load_iris()\n","\n","# Description About data set\n","print(iris.DESCR)\n","# After seeing below we need to build a model for classifying the classes of iris, so it is classification Problem."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"jtbcfnjaUBxd","outputId":"e75e28f3-d5fe-44e7-e841-76bee5fd54da","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667310840362,"user_tz":-330,"elapsed":18,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n","0                5.1               3.5                1.4               0.2\n","1                4.9               3.0                1.4               0.2\n","2                4.7               3.2                1.3               0.2\n","3                4.6               3.1                1.5               0.2\n","4                5.0               3.6                1.4               0.2\n","   species\n","0        0\n","1        0\n","2        0\n","3        0\n","4        0\n"]}],"source":["# Training Data\n","train = pd.DataFrame(iris.data,columns = iris.feature_names)\n","\n","# Testing Data\n","target = pd.DataFrame(iris.target,columns = ['species'])\n","\n","print(train.head())\n","print(target.head())"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"--5YVCoLUBxg","executionInfo":{"status":"ok","timestamp":1667310840362,"user_tz":-330,"elapsed":14,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[],"source":["X_train,X_test,y_train,y_test =  train_test_split(train,\n","                                                  target,\n","                                                  test_size=0.2,\n","                                                  random_state=10)\n","\n","model = XGBClassifier(random_state=6,n_estimators=10)\n","\n","model.fit(X_train,y_train)\n","y_predict = model.predict(X_test)"]},{"cell_type":"code","source":["skf = StratifiedKFold(n_splits=10)\n","\n","Stratified_score = []\n","for train_index, test_index in skf.split(train, target):\n","    \n","    X_train, X_test = train.iloc[list(train_index),:], train.iloc[list(test_index),:]\n","    y_train, y_test = target.iloc[list(train_index),:], target.iloc[list(test_index),:]\n","    \n","    model = XGBClassifier()\n","    model.fit(X_train,y_train)\n","    y_predict = model.predict(X_test)\n","    Stratified_score.append(accuracy_score(y_test,y_predict))"],"metadata":{"id":"eVuPA3XdbLz7","executionInfo":{"status":"ok","timestamp":1667310841046,"user_tz":-330,"elapsed":696,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"id":"amzOuMJQUBxj","outputId":"26d05e5d-2cd1-497f-9a65-de333932b7d5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667310841047,"user_tz":-330,"elapsed":30,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Minimum accuracy we get is 0.9333333333333333\n","Maximun accuracy we get is 1.0\n","We can get average accuracy is 0.96\n","Accuracy of random forest tree model for classifying iris species 1.0\n"]}],"source":["print(\"Minimum accuracy we get is {}\".format(min(Stratified_score)))\n","print(\"Maximun accuracy we get is {}\".format(max(Stratified_score)))\n","print(\"We can get average accuracy is {}\".format(\n","    statistics.mean(Stratified_score)))\n","\n","print(\"Accuracy of random forest tree model for classifying iris species\",\n","      accuracy_score(y_test,y_predict))"]},{"cell_type":"markdown","metadata":{"id":"YwddfvsEUOxK"},"source":["<h3>HyperParameter Tunning</h3>\n","\n","\n","**min_samples_split**\n","- Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n","- Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n","- Too high values can lead to under-fitting hence, it should be tuned using CV.\n","\n","**min_samples_leaf**\n","- Defines the minimum samples (or observations) required in a terminal node or leaf.\n","- Used to control over-fitting similar to min_samples_split.\n","- Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.\n","\n","**min_weight_fraction_leaf**\n","- Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer.\n","\n","**max_depth**\n","- The maximum depth of a tree.\n","- Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample. Should be tuned using CV.\n","\n","**max_leaf_nodes**\n","- The maximum number of terminal nodes or leaves in a tree.\n","- Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n","- If this is defined, GBM will ignore max_depth.\n","\n","**max_features**\n","- The number of features to consider while searching for a best split. These will be randomly selected.\n","- As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n","- Higher values can lead to over-fitting but depends on case to case.\n","\n","\n","\n","**learning_rate**\n","- This determines the impact of each tree on the final outcome (step 2.4). GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates.\n","- Lower values are generally preferred as they make the model robust to the specific characteristics of tree and thus allowing it to generalize well.\n","- Lower values would require higher number of trees to model all the relations and will be computationally expensive.\n","\n","**n_estimators**\n","- The number of sequential trees to be modeled (step 2)\n","- Though GBM is fairly robust at higher number of trees but it can still overfit at a point. Hence, this should be tuned using CV for a particular learning rate.\n","\n","**subsample**\n","- The fraction of observations to be selected for each tree. Selection is done by random sampling.\n","- Values slightly less than 1 make the model robust by reducing the variance.\n","- Typical values ~0.8 generally work fine but can be fine-tuned further.\n","\n","**loss**\n","- It refers to the loss function to be minimized in each split.\n","- It can have various values for classification and regression case. Generally the default values work fine. Other values should be chosen only if you understand their impact on the model."]},{"cell_type":"code","source":["# Learning rate for gradient boosting\n","learning_rates = [0.1, 0.05, 0.001, 0.01]\n","\n","# Number of trees in random forest\n","n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n","\n","# Maximum number of levels in tree\n","max_depths = [5,6,7,8,9,10]\n","\n","# Create the random grid\n","random_grid = {'learning_rate': learning_rates,\n","               'n_estimators': n_estimators,\n","               'max_depth': max_depths}\n","print(random_grid)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXbODRlCchLs","executionInfo":{"status":"ok","timestamp":1667310841047,"user_tz":-330,"elapsed":16,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}},"outputId":"5942e52b-1de4-4c82-ffa5-0ce2fb6f9345"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["{'learning_rate': [0.1, 0.05, 0.001, 0.01], 'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200], 'max_depth': [5, 6, 7, 8, 9, 10]}\n"]}]},{"cell_type":"code","source":["random_search = RandomizedSearchCV(XGBClassifier(objective = 'multi:softmax', \n","                                                 eval_metric = 'merror',\n","                                                 subsample = 0.8,\n","                                                 colsample_bytree = 0.8), \n","                                   random_grid, \n","                                   random_state=1, \n","                                   n_iter=100, \n","                                   cv=5, \n","                                   verbose=0, \n","                                   n_jobs=-1)\n","\n","random_search.fit(X_train,y_train)\n","\n","#Print The value of best Hyperparameters\n","print(random_search.best_params_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1Px1mJ1btaZ","executionInfo":{"status":"ok","timestamp":1667310852548,"user_tz":-330,"elapsed":11514,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}},"outputId":"9db7cd36-19ae-4535-b3bf-bec3434d23c0"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["{'n_estimators': 64, 'max_depth': 7, 'learning_rate': 0.05}\n"]}]},{"cell_type":"code","source":["from pprint import pprint\n","model = XGBClassifier(objective = 'multi:softmax' , \n","                      eval_metric = 'merror',\n","                      subsample = 0.8,\n","                      colsample_bytree = 0.8,\n","                      n_estimators = 64, \n","                      max_depth = 7, \n","                      learning_rate = 0.05)\n","\n","model.fit(X_train,y_train)\n","\n","y_predict = model.predict(X_test)\n","y_proba = model.predict_proba(X_test)\n","print(\"Accuracy of random forest tree model for classifying iris species\",\n","      accuracy_score(y_test,y_predict))\n","\n","params = model.get_params()\n","print(\"Currently used params\\n\\n\",params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-gEbHY6cd9r","executionInfo":{"status":"ok","timestamp":1667310852549,"user_tz":-330,"elapsed":16,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}},"outputId":"0d27c223-5107-4be6-ed1b-04bd45033fed"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of random forest tree model for classifying iris species 1.0\n","Currently used params\n","\n"," {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.05, 'max_delta_step': 0, 'max_depth': 7, 'min_child_weight': 1, 'missing': None, 'n_estimators': 64, 'n_jobs': 1, 'nthread': None, 'objective': 'multi:softprob', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': None, 'silent': None, 'subsample': 0.8, 'verbosity': 1, 'eval_metric': 'merror'}\n"]}]}]}