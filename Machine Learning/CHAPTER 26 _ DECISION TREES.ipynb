{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOOZotLq/KlPG2tkileJYa7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**<h1><center>DECISION TREES</center></h1>**\n","\n","Decision Trees is a Supervised Machine Learning Algorithm. It is used for both classification and Regression Problem. Decision tree uses the tree representation to solve the problem in which each leaf node corresponds to a class label and attributes are represented on the internal node of the tree.\n","\n","Below diagram illustrate the basic flow of decision tree for decision making with labels (Rain(Yes), No Rain(No)).\n","\n","<center><img src=\"https://miro.medium.com/max/875/0*PB7MYQfzyaLaTp1n\" width = 400px></center>\n","\n","**<u>Decision Tree Terminologies</u>**\n","\n","1. <u>Root Node</u> :  This is node where decision tree starts. It represents the entire dataset, which further gets divided into two or more homogeneous sets.\n","\n","2. <u>Leaf Node</u> : Leaf node is final output node. This node represents class labels.\n","\n","3. <u>Splitting</u>: Splitting is the process of dividing the decision node/root node into sub-nodes according to the given conditions.\n","\n","4. <u>Branch/Sub Tree</u>: A tree formed by splitting the tree.\n","\n","5. <u>Pruning</u>: Pruning is the process of removing the unwanted branches from the tree.\n","\n","6. <u>Parent/Child node</u>: The root node of the tree is called the parent node, and other nodes are called the child nodes.\n","\n","\n","\n","Trees models used for both classification and regression tasks. CART(Classification and Regression Trees) is general Term used for this.\n","\n","\n","**<u><h2>Approach to make decision Trees</h2></u>**\n","\n","In decision trees major problem is to find attribute for root node at each level. \n","\n","<u>Their are two approach:</u>\n","\n","1. Information Gain\n","\n","2. Gini Index\n","\n","\n","**<u>1. Information Gain</u>**\n","\n","Information gain  measures how well a given attribute separates the training examples according to their target classification.  The split with the highest information gain will be taken as the first split and the process will continue until all children nodes are pure, or until the information gain is 0.\n","\n","<img src=\"https://s3-ap-southeast-1.amazonaws.com/he-public-data/high%20information%20gaine8d3940.png\" width = 300px>\n","\n","\n","<img src=\"https://s3-ap-southeast-1.amazonaws.com/he-public-data/low%20information%20gainef0016c.png\" width = 300px>\n","\n","Information calculated using entropy. Suppose S is a set of instances, A is an attribute, Sv is the subset of S with A = v, and Values (A) is the set of all possible values of A, then\n","\n","<img src=\"https://miro.medium.com/max/1400/0*2CpXDgzKNzeJ3Tix.png\">\n","\n","where Values(A) is the set of all possible values for attribute the A, and $S_v$ is the subset of S for which attribute A has value v. Note the first term in the equation is just entropy of the original sample S, and the second term is the expected value of entropy after S is partitioned using attribute A, i.e. entropy of its children.\n","\n","<u>Entropy</u> : Entropy is the measure of uncertainty of a random variable. \n","\n","`Entropy = -p * log2(p)`\n","\n","**<u>Lets understand Information gain calculation using example:</u>**\n","\n","Now, lets draw a Decision Tree for the following data using Information gain.\n","\n","| X    | Y    | Z    | C    |\n","|------|------|------|------|\n","|   1  |  1   |   1  | I    |\n","|------|------|------|------|\n","| 1    | 1    | 0    | I    |\n","|------|------|------|------|\n","|   0  | 0    |   1  | II   |\n","|----  |------|------|------|\n","|   1  | 0    |   0  | II   |\n","\n","\n","\n","\n","Here, we have 3 features and 2 output classes.\n","To build a decision tree using Information gain. We will take each of the feature and calculate the information for each feature.\n","\n","#### <center>Split on feature X</center>\n","\n","\n","Entropy for parent is \n","\n","    - [(2/4) * log2(2/4) +  (2/4) * log2(2/4)] = -[0.5 * (-1) + 0.5 * (-1)] \n","    = -[-0.5 + -0.5] = -[-1] = 1\n","    \n","Entropy for left child is \n","\n","    - [(1/1) * log2(1/1)] = -[0.5 * 0] \n","    = -[0] = 0\n","    \n","Entropy for right child is \n","\n","    - [(2/3) * log2(2/3) +  (1/3) * log2(1/3)]\n","    = 0.9184\n","    \n","    \n","Information Gain for root node x is \n","\n","            = Entropy of root node - [summation(p(classes/total classes) * Entroy of childs)]\n","            = 1 - [summation(p(classes/total classes) * Entroy of childs)]\n","            = 1 - [3/4(0.9184) + 1/4 * (0)\n","            = 0.3192\n","            \n","            \n","#### <center>Split on feature Y</center>\n","\n","\n","Entropy for parent is \n","\n","    - [(2/4) * log2(2/4) +  (2/4) * log2(2/4)] = -[0.5 * (-1) + 0.5 * (-1)] \n","    = -[-0.5 + -0.5] = -[-1] = 1\n","    \n","Entropy for left child is \n","\n","    - [(2/2) * log2(2/2) +  (2/2) * log2(2/2)] = -[1 * 0 + 1 * 0] \n","    = -[0] = 0\n","    \n","Entropy for right child is \n","\n","    - [(2/2) * log2(2/2) +  (2/2) * log2(2/2)]\n","    = 0\n","    \n","    \n","Information Gain for root node Y is \n","\n","            = Entropy of root node - [summation(p(classes/total classes) * Entroy of childs)]\n","            = 1 - [summation(p(classes/total classes) * Entroy of childs)]\n","            = 1 - [2/4 * (0) + 2/4 * (0)\n","            = 1\n","\n","#### <center>Split on feature z</center>\n","\n","Entropy for parent is \n","\n","    - [(2/4) * log2(2/4) +  (2/4) * log2(2/4)] = -[0.5 * (-1) + 0.5 * (-1)] \n","    = -[-0.5 + -0.5] = -[-1] = 1\n","    \n","Entropy for left child is \n","\n","    - [(1/2) * log2(1/2) +  (1/2) * log2(1/2)] = -[0.5 * -1 + 0.5 * 1] \n","    = -[-0.5 + -0.5] = 1\n","    \n","Entropy for right child is \n","\n","    - [(1/2) * log2(1/2) +  (1/2) * log2(1/2)] = -[0.5 * -1 + 0.5 * 1]\n","    = -[-0.5 + -0.5] = 1\n","    \n","    \n","Information Gain for root node z is \n","\n","            = Entropy of root node - [summation(p(classes/total classes) * Entroy of childs)]\n","            = 1 - [summation(p(classes/total classes) * Entroy of childs)]\n","            = 1 - [2/4 * (1) + 2/4 * (1)]\n","            = 1 - 1\n","            = 0\n","\n","\n","From the above calculations we can see that the information gain is maximum when we make a split on feature Y. So, for the root node best suited feature is feature Y. Now we can see that while splitting the dataset by feature Y, the child contains pure subset of the target variable. So we don’t need to further split the dataset.\n","\n","**<u>2. Gini Impurity</u>**\n","\n","Before getting into Gini Impurity lets discuss Pure and Impure. \n","\n","<u>Pure</u> : Pure means, in a selected sample of dataset all data belongs to same class.\n","\n","<u>Impure</u> : Impure means, data is mixture of different classes.\n","\n","Gini Index or Gini Impurity is a metric to measure how often a randomly chosen element would be incorrectly identified.It means an attribute with lower Gini index should be preferred.\n","\n","Sklearn supports `Gini` criteria for Gini Index and by default, it takes `gini` value.\n","                \n","`Gini Index= 1- ∑jPj2`\n","\n","\n","               \n","- Example:\n","Lets consider the dataset in the image below and draw a decision tree using gini index.\n","\n","|A     | B    | C    | D    | Target   |\n","|------|------|------|------|----------|\n","| 4.8  | 3.4  | 1.9  | 0.2  | positive |\n","|------|------|------|------|----------|\n","| 5    | 3    | 1.6  | 1.2  | positive |\n","|------|------|------|------|----------|\n","|   5  | 3.4  | 1.6  | 0.2  | positive |\n","|----  |------|------|------|----------|\n","| 5.2  | 3.5  | 1.5  | 0.2  | positive |\n","|------|------|------|------|----------|\n","| 5.2  | 3.4  | 1.4  | 0.2  | positive |\n","|------|------|------|------|----------|\n","| 4.7  | 3.2  | 1.6  | 0.2  | positive |\n","|------|------|------|------|----------|\n","| 4.8  | 3.1  | 1.6  | 0.2  | positive |\n","|----  |------|------|------|----------|\n","| 5.4  | 3.4  | 1.5  | 0.4  | positive |\n","|------|------|------|------|----------|\n","|   7  | 3.2  | 4.7  | 1.4  | negative |\n","|------|------|------|------|----------|\n","| 6.4  | 3.2  | 4.7  | 1.5  | negative |\n","|------|------|------|------|----------|\n","| 6.9  | 3.1  | 4.9  | 1.5  | negative |\n","|------|------|------|------|----------|\n","| 5.5  | 2.3  |   4  | 1.3  | negative |\n","|----  |------|------|------|----------|\n","| 6.5  | 2.8  | 4.6  | 1.5  | negative |\n","|------|------|------|------|----------|\n","| 5.7  | 2.8  | 4.5  | 1.3  | negative |\n","|------|------|------|------|----------|\n","| 6.3  | 3.3  | 4.7  | 1.6  | negative |\n","|------|------|------|------|----------|\n","| 4.9  | 2.4  | 3.3  | 1    | negative |\n","\n","\n","In the dataset above there are 5 attributes from which attribute target is the predicting feature which contains 2(Positive & Negative) classes. We have an equal proportion for both the classes.In Gini Index, we have to choose some random values to categorize each attribute. \n","\n","These values for this dataset are:\n","\n","                 A       B        C         D\n","              >= 5     >= 3.0      >= 4.2    >= 1.4\n","               < 5      < 3.0       < 4.2     < 1.4\n","\n","- Calculating Gini Index for Var A:\n","        Value >= 5\n","                        Attribute A >= 5 & class = positive: 5/12\n","                        Attribute A >= 5 & class = negative: 7/12\n","                        \n","        Gini(5, 7) = 1 – [(5/12)^2 + (7/12)^2] = 0.4860\n","                        \n","\n","        Value < 5\n","                        Attribute A < 5 & class = positive: 3/4\n","                        Attribute A < 5 & class = negative: 1/4\n","                        \n","        Gini(3, 1) = 1 – [(3/4)^2 + (1/4)^2] = 0.375\n","        \n","        By adding weight and sum each of the gini indices:\n","                \n","                gini(Target,A) = (12/16) * 0.486 + (4/16) * 0.375\n","                               = 0.45825\n","                               \n","- Calculating Gini Index for Var B:\n","        Value >= 3\n","                        Attribute A >= 5 & class = positive: 8/12\n","                        Attribute A >= 5 & class = negative: 4/12\n","                        \n","        Gini(8, 4) = 1 – [(8/12)^2 + (4/12)^2] = 0.4460\n","                        \n","\n","        Value < 3\n","                        Attribute A < 5 & class = positive: 0/4\n","                        Attribute A < 5 & class = negative: 4/4\n","                        \n","        Gini(0, 4) = 1 – [(3/4)^2 + (1/4)^2] = 0\n","        \n","        By adding weight and sum each of the gini indices:\n","                \n","                gini(Target,B) = (12/16) * 0.446 + (4/16) * 0\n","                               = 0.45825\n","\n","Using the same approach we can calculate the Gini index for C and D attributes.\n","\n","                            Positive    Negative\n","\n","            For C|>= 4.2       0          6\n","\n","                 |< 4.2        8          2\n","\n","                Gini Index of C= 0.2   \n","\n","\n","                            Positive    Negative\n","            For D|>= 1.4        0         5\n","                 |< 1.4         8         3\n","                Gini Index of D= 0.273\n","\n","\n","So Among A, B,C and D ,  C feature got less value in gini index so we use C feature to split at root node. \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"PPl-YSEYp3A2"}}]}