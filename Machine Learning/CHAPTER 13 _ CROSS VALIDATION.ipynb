{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN9PX4THW+gHTeH1vQNQAQV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**<h1><center>CROSS-VALIDATION</center></h1>**\n","\n","Cross-validation is a technique to evaluate the model performance by dividing the data into two parts, model will train on one part and test on another part, this process iterates on every possible combination.\n","\n","<u>Cross Validation Techniques</u>\n","\n","1. LOOCV (Leave One Out Cross Validation)\n","2. K-Fold Cross Validation\n","3. Stratifies Cross Validation\n","\n","**<u>1. LOOCV (Leave One Out Cross Validation) : </u>**\n","\n","In this method, we will train on the whole dataset but leaves only one data point for testing and then it iterates for each data point. It has some advantages as well as disadvantages also.\n","- An advantage of using this method is that we make use of all data points and hence it is low bias.\n","\n","- The major drawback  is it takes a lot of execution time as it iterates over `the number of data points` times and it leads to higher variation in the testing model as we are testing against only one data point. If the data point is an outlier it can lead to higher variation. \n","\n","**<u>2. K-Fold Cross Validation : </u>**\n","\n","K-Fold CV is where a given data set is split into a K number of sections/folds where each fold is used as a testing set at some point. \n","\n","Lets take one example that data set will be divided into 5 parts called 5-Fold cross validation where `K=5`. In the first iteration, the first fold is used to test the model and the rest are used to train the model. In the second iteration, 2nd fold is used as the testing set while the rest serve as the training set. This process is repeated until each fold of the 5 folds have been used as the testing set.\n","\n","<center><img src = \"https://miro.medium.com/max/1400/1*9NosjiPCNNAhHfEdNYFfUQ.png\" width = 450px></center>\n","\n","**Evaluating a ML model using K-Fold CV**"],"metadata":{"id":"3SXzw64VtEt8"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import KFold\n","\n","X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n","y = np.array([0, 0, 1, 1])\n","kf = KFold(n_splits=2)\n","\n","for train, test in kf.split(X,y):\n","     print(\"%s %s\" % (train, test))"],"metadata":{"id":"GQxNmUeu4Ifv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667282588094,"user_tz":-330,"elapsed":670,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}},"outputId":"1420de0c-b20e-42e2-b066-2c1cbc6515df"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[2 3] [0 1]\n","[0 1] [2 3]\n"]}]},{"cell_type":"markdown","source":["Here we have labels 0,1 so the k-fold cross-validation spit choosed is 2 and 3rd index for training , that is label 1 which is imbalanced data set.To avoid this we use stratified cross validation."],"metadata":{"id":"ErXmGQ8K4Ok-"}},{"cell_type":"markdown","source":["**<u>3. Stratifies Cross Validation</u>**\n","\n","In some cases we will have imbalanced datset while splitting dataset into train and test we should concern about target classes. Stratifies cross validation is extension to k-fold cross validation.\n","\n","In this technique each fold contains approximately the same percentage of samples of each target class as the complete set, or in case of prediction problems, the mean response value is approximately equal in all the folds. This variation is also known as Stratified K Fold."],"metadata":{"id":"3UAxvlGW60UM"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","\n","\n","X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n","y = np.array([0, 0, 1, 1])\n","\n","strtifieskfold = StratifiedKFold(n_splits=2)\n","print(strtifieskfold)\n","strtifieskfold.get_n_splits(X, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3Q7OfmE9Hy1","executionInfo":{"status":"ok","timestamp":1662453626044,"user_tz":-330,"elapsed":532,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}},"outputId":"7488ce46-65e2-4419-b1d2-cce2de8d1d34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n"]},{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["\n","for train_index, test_index in strtifieskfold.split(X, y):\n","    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITYlqAse9Pic","executionInfo":{"status":"ok","timestamp":1662453653379,"user_tz":-330,"elapsed":82,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}},"outputId":"e0d039a7-3b13-4570-cd04-da6115330626"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN: [1 3] TEST: [0 2]\n","TRAIN: [0 2] TEST: [1 3]\n"]}]},{"cell_type":"markdown","source":["Here we have labels 0,1 so the k-fold cross-validation spit choosed is 1 and 3rd index for training , that is label 0 and 1 which is balanced data set."],"metadata":{"id":"cCohKPS09ivs"}}]}