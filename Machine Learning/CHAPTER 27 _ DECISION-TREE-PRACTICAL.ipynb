{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPbMEFd+BgaWrfEHG66zSv+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"j5RcoMnNV5lX"},"source":["**<center><h1>Decision Tree</h1></center>**\n","\n","We will use the scikit-learn library to build the decision tree model. We will be using the iris dataset to build a decision tree classifier. The data set contains information of 3 classes of the iris plant with the following attributes: \n","    - sepal length \n","    - sepal width \n","    - petal length \n","    - petal width \n","- class: \n","\n","        Iris Setosa \n","        Iris Versicolour \n","        Iris Virginica\n","\n","The task is to predict the class of the iris plant based on the attributes."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"diTkMzZJV5lg","executionInfo":{"status":"ok","timestamp":1667300787107,"user_tz":-330,"elapsed":1540,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[],"source":["#Importing required libraries\n","import pandas as pd\n","import numpy as np\n","import statistics\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.datasets import load_iris\n","\n","from sklearn.tree import DecisionTreeClassifier\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import KFold\n","\n","from sklearn.metrics import accuracy_score,confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","from pprint import pprint\n","\n","from pprint import pprint"]},{"cell_type":"markdown","metadata":{"id":"4_PxYREkV5ll"},"source":["The scikit-learn dataset library already has the iris dataset. You can either use the dataset from the source or import it from the scikit-learn dataset library."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QRlDQ_QuV5ln","outputId":"4d50a10d-b9d5-45a9-dbb1-f1e4fd6e96dc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300787111,"user_tz":-330,"elapsed":189,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Classes to predict:  ['setosa' 'versicolor' 'virginica']\n"]}],"source":["#Loading the iris data\n","data = load_iris()\n","print('Classes to predict: ', data.target_names)"]},{"cell_type":"markdown","metadata":{"id":"1_qy2HytV5lp"},"source":["There are three classes of iris plants: 'setosa', 'versicolor' and 'virginica'. Now, we have imported the iris data in the variable 'data'. We will now extract the attribute data and the corresponding labels. We can extract the attributes and labels by calling .data and .target as shown below:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Iw5xWoWtV5lq","outputId":"7a2f42ca-5779-425b-cae5-8efd70a50fd1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300787114,"user_tz":-330,"elapsed":164,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of examples in the data: 150\n"]}],"source":["# Training Data\n","X = pd.DataFrame(data.data,columns = data.feature_names)\n","\n","# Testing Data\n","y = pd.DataFrame(data.target,columns = ['species'])\n","\n","print('Number of examples in the data:', X.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"1rwWGERvV5ls"},"source":["There are 150 examples/ samples in the data. The variable 'X' contains the attributes to the iris plant. The cell below shows the 4 attributes of the first four iris plants.\n","\n","Now that we have extracted the data attributes and corresponding labels, we will split them to form train and test datasets. For this purpose, we will use the scikit-learn's 'train_test_split' function, which takes in the attributes and labels as inputs and produces the train and test sets."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"e-UcmpuQV5lt","executionInfo":{"status":"ok","timestamp":1667300787118,"user_tz":-330,"elapsed":149,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[],"source":["#Using the train_test_split to create train and test sets.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 47, test_size = 0.25)"]},{"cell_type":"markdown","metadata":{"id":"9-WclJ4eV5lv"},"source":["Since, this is a classification problem, we will import the DecisionTreeClassifier function from the sklearn library. Next, we will set the 'criterion' to 'entropy', which sets the measure for splitting the attribute to information gain."]},{"cell_type":"markdown","metadata":{"id":"qG8FxQf4V5lx"},"source":["<h3>Training the decision tree using entropy</h3>"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Ut7bGtU9V5lz","executionInfo":{"status":"ok","timestamp":1667300787121,"user_tz":-330,"elapsed":146,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[],"source":["#Importing the Decision tree classifier from the sklearn library.\n","from sklearn.tree import DecisionTreeClassifier\n","clf = DecisionTreeClassifier(criterion = 'entropy')"]},{"cell_type":"markdown","metadata":{"id":"KABmEe8LV5l0"},"source":["Next, we will fit the classifier on the train attributes and labels."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"czLXKo2_V5l1","executionInfo":{"status":"ok","timestamp":1667300787128,"user_tz":-330,"elapsed":149,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[],"source":["#Training the decision tree classifier. \n","clf.fit(X_train, y_train)\n","\n","#Predicting labels on the test set.\n","y_pred =  clf.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"-cB5sJcVV5l3"},"source":["We will now evaluate the predicted classes using some metrics. For this case, we will use 'accuracy_score' to calculate the accuracy of the predicted labels."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"1XCtFK_qV5l3","outputId":"4aebad29-e4c9-4a07-80ea-c708d70d6411","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300787131,"user_tz":-330,"elapsed":145,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score on train data:  1.0\n","Accuracy Score on test data: 0.9473684210526315\n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        15\n","           1       0.80      1.00      0.89         8\n","           2       1.00      0.87      0.93        15\n","\n","    accuracy                           0.95        38\n","   macro avg       0.93      0.96      0.94        38\n","weighted avg       0.96      0.95      0.95        38\n","\n"]}],"source":["#Importing the accuracy metric from sklearn.metrics library\n","print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n","print('Accuracy Score on test data: {}\\n\\n'.format(accuracy_score(y_true=y_test, y_pred=y_pred)))\n","print(classification_report(y_test,y_pred))"]},{"cell_type":"markdown","metadata":{"id":"wqAz2GvZV5l7"},"source":["<h3>Training the decision tree using gini</h3>"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"S4Ax8bNMV5l8","executionInfo":{"status":"ok","timestamp":1667300787135,"user_tz":-330,"elapsed":138,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[],"source":["#Importing the Decision tree classifier from the sklearn library.\n","clf = DecisionTreeClassifier(criterion = 'gini')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"SrAq9SCKV5l_","executionInfo":{"status":"ok","timestamp":1667300787139,"user_tz":-330,"elapsed":137,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[],"source":["#Training the decision tree classifier. \n","clf.fit(X_train, y_train)\n","\n","#Predicting labels on the test set.\n","y_pred =  clf.predict(X_test)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Fmzn0qN2V5mA","outputId":"c5afc89d-0d61-44e7-aa4a-dfe782cc2505","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300787144,"user_tz":-330,"elapsed":139,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score on train data:  1.0\n","Accuracy Score on test data: 0.9473684210526315\n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        15\n","           1       0.80      1.00      0.89         8\n","           2       1.00      0.87      0.93        15\n","\n","    accuracy                           0.95        38\n","   macro avg       0.93      0.96      0.94        38\n","weighted avg       0.96      0.95      0.95        38\n","\n"]}],"source":["#Importing the accuracy metric from sklearn.metrics library\n","print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n","print('Accuracy Score on test data: {}\\n\\n'.format(accuracy_score(y_true=y_test, y_pred=y_pred)))\n","print(classification_report(y_test,y_pred))"]},{"cell_type":"markdown","metadata":{"id":"C9gS3sqiV6XL"},"source":["<h3>K-Fold Cross Validation</h3>"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"YjC4mlhzV6XU","executionInfo":{"status":"ok","timestamp":1667300787147,"user_tz":-330,"elapsed":126,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[],"source":["kf = KFold(n_splits=10)\n","\n","k_fold_score = []\n","for train_index, test_index in kf.split(X,y):\n","    \n","    # print(X.iloc[list(train_index),:])\n","    \n","    X_train, X_test = X.iloc[list(train_index),:], X.iloc[list(test_index),:]\n","    y_train, y_test = y.iloc[list(train_index),:], y.iloc[list(test_index),:]\n","    \n","    model = DecisionTreeClassifier()\n","    model.fit(X_train,y_train)\n","    y_predict = model.predict(X_test)\n","    k_fold_score.append(accuracy_score(y_test,y_predict))\n","   "]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Dex0NE0jV6XZ","outputId":"86d2370c-60d0-48d2-b297-db4bc646228d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300787150,"user_tz":-330,"elapsed":125,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Minimum accuracy we get is 0.8\n","Maximun accuracy we get is 1.0\n","We can get average accuracy is 0.9400000000000001\n"]}],"source":["print(\"Minimum accuracy we get is {}\".format(min(k_fold_score)))\n","print(\"Maximun accuracy we get is {}\".format(max(k_fold_score)))\n","print(\"We can get average accuracy is {}\".format(statistics.mean(k_fold_score)))"]},{"cell_type":"markdown","metadata":{"id":"Eh_Xsd70V6Xb"},"source":["<h3>Stratified K Fold Cross Validation</h3>"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"LU-_gqN1V6Xc","executionInfo":{"status":"ok","timestamp":1667300787152,"user_tz":-330,"elapsed":116,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[],"source":["skf = StratifiedKFold(n_splits=10)\n","\n","Stratified_score = []\n","for train_index, test_index in skf.split(X, y):\n","    \n","    X_train, X_test = X.iloc[list(train_index),:], X.iloc[list(test_index),:]\n","    y_train, y_test = y.iloc[list(train_index),:], y.iloc[list(test_index),:]\n","    \n","    model = DecisionTreeClassifier()\n","    model.fit(X_train,y_train)\n","    y_predict = model.predict(X_test)\n","    Stratified_score.append(accuracy_score(y_test,y_predict))"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"1pwhygx_V6Xf","outputId":"184b789f-8027-428e-e4c7-ab535e7e0eee","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300787155,"user_tz":-330,"elapsed":115,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Minimum accuracy we get is 0.8666666666666667\n","Maximun accuracy we get is 1.0\n","We can get average accuracy is 0.9533333333333334\n"]}],"source":["print(\"Minimum accuracy we get is {}\".format(min(Stratified_score)))\n","print(\"Maximun accuracy we get is {}\".format(max(Stratified_score)))\n","print(\"We can get average accuracy is {}\".format(statistics.mean(Stratified_score)))"]},{"cell_type":"markdown","metadata":{"id":"w9Dm5YiMV8gg"},"source":["<h3><center>Parameter Tunning</center></h3>\n","\n","<u><h4>Grid Search</h4></u>"]},{"cell_type":"markdown","metadata":{"id":"h_esiXAHV8gi"},"source":["<h4>Parameters</h4>\n","\n","1. **criterion : {“gini”, “entropy”}, default=”gini”:**\n","\n","    The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n","        \n","\n","\n","2. **max_depth : int, default=None**\n","\n","    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n","\n","\n","3. **min_samples_split : int or float, default=2**\n","\n","    The minimum number of samples required to split an internal node:\n","\n","    If int, then consider min_samples_split as the minimum number.\n","\n","    If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n","    \n","\n","4. **max_features : float or {“auto”, “sqrt”, “log2”}, default=None**\n","\n","    The number of features to consider when looking for the best split:\n","\n","    If int, then consider max_features features at each split.\n","\n","    If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n","\n","    If “auto”, then max_features=sqrt(n_features).\n","\n","    If “sqrt”, then max_features=sqrt(n_features).\n","\n","    If “log2”, then max_features=log2(n_features).\n","\n","    If None, then max_features=n_features.\n","    \n","    Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n","    \n","\n","5. **min_samples_leaf : int or float, default=1**\n","\n","    The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n","\n","    If int, then consider min_samples_leaf as the minimum number.\n","\n","    If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"KqheVFzVV8gk","outputId":"daa8a462-2e6a-4e6d-9ed2-25daa547748a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300787159,"user_tz":-330,"elapsed":110,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'criterion': ['gini', 'entropy'],\n"," 'max_depth': [3, None],\n"," 'max_features': ['auto', 'sqrt'],\n"," 'min_samples_leaf': [1, 2, 4]}\n"]}],"source":["param_grid = {\"max_depth\": [3, None],\n","              \"min_samples_leaf\": [1, 2, 4],\n","              \"criterion\": [\"gini\", \"entropy\"],\n","              \"max_features\": ['auto', 'sqrt']} \n","\n","pprint(param_grid)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"_AGyULtsV8gl","outputId":"b7bba847-a275-41b0-ec9f-b3ff7680954e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300790015,"user_tz":-330,"elapsed":2958,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 24 candidates, totalling 72 fits\n","Best Parameters {'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1}\n"]}],"source":["rf = DecisionTreeClassifier()\n","\n","rf_random = GridSearchCV(rf, \n","                         param_grid, \n","                         cv = 3, \n","                         verbose=2, \n","                         n_jobs = -1)\n","\n","rf_random.fit(X_train,y_train)\n","\n","print(\"Best Parameters\",rf_random.best_params_)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"x2beR3AAV8gm","outputId":"df63998b-83db-4fad-a48d-fa307459a5fc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300790018,"user_tz":-330,"elapsed":114,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score on train data:  1.0\n","Accuracy Score on test data: 0.9333333333333333\n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00         5\n","           1       0.83      1.00      0.91         5\n","           2       1.00      0.80      0.89         5\n","\n","    accuracy                           0.93        15\n","   macro avg       0.94      0.93      0.93        15\n","weighted avg       0.94      0.93      0.93        15\n","\n"]}],"source":["#Importing the Decision tree classifier from the sklearn library.\n","clf = DecisionTreeClassifier(criterion = 'gini',\n","                            max_depth = None,\n","                            max_features = 'auto',\n","                            min_samples_leaf = 1)\n","\n","\n","#Training the decision tree classifier. \n","clf.fit(X_train, y_train)\n","\n","#Predicting labels on the test set.\n","y_pred =  clf.predict(X_test)\n","\n","\n","#Importing the accuracy metric from sklearn.metrics library\n","\n","from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n","\n","print('Accuracy Score on train data: ', \n","      accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n","print('Accuracy Score on test data: {}\\n\\n'.format(accuracy_score(y_true=y_test, \n","                                                                  y_pred=y_pred)\n","                                                                  ))\n","print(classification_report(y_test,y_pred))"]},{"cell_type":"markdown","metadata":{"id":"kdgZ8C9gV8gn"},"source":["<h4><u>Random Search</u></h4>"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"t9R6EorBV8gn","outputId":"aefed4a2-847d-4a87-af90-0c71e4b7ea2f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300790021,"user_tz":-330,"elapsed":109,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 24 candidates, totalling 72 fits\n","{'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 1}\n"]}],"source":["random_search = RandomizedSearchCV(DecisionTreeClassifier(), \n","                                   param_grid, \n","                                   random_state=1, \n","                                   n_iter=100, \n","                                   cv=5, \n","                                   verbose=0, \n","                                   n_jobs=-1)\n","\n","rf_random.fit(X_train,y_train)\n","\n","#Print The value of best Hyperparameters\n","print(rf_random.best_params_)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"ww4Idc-_V8gp","outputId":"c2f0ae73-f719-4cb4-89fd-f3ba29312a4e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667300790023,"user_tz":-330,"elapsed":94,"user":{"displayName":"Santhosh Kumar","userId":"02779163303914785334"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score on train data:  0.9703703703703703\n","Accuracy Score on test data: 1.0\n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00         5\n","           1       1.00      1.00      1.00         5\n","           2       1.00      1.00      1.00         5\n","\n","    accuracy                           1.00        15\n","   macro avg       1.00      1.00      1.00        15\n","weighted avg       1.00      1.00      1.00        15\n","\n"]}],"source":["clf = DecisionTreeClassifier(criterion = 'gini',\n","                            max_depth = None,\n","                            max_features = 'auto',\n","                            min_samples_leaf = 4)\n","\n","\n","#Training the decision tree classifier. \n","clf.fit(X_train, y_train)\n","\n","#Predicting labels on the test set.\n","y_pred =  clf.predict(X_test)\n","\n","\n","print('Accuracy Score on train data: ', \n","      accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n","print('Accuracy Score on test data: {}\\n\\n'.format(\n","    accuracy_score(y_true=y_test, y_pred=y_pred)))\n","print(classification_report(y_test,y_pred))"]}]}