{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM3LmpErm0K56TQJxMcCcHi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**<h1><center>ENSEMBLE LEARNING</center></h1>**\n","\n","Ensemble Learning algorithms combines several decision trees to produce better results. The main idea behind Ensemble Learning is that a group of weak learners come together to form a strong learner.\n","\n","<U>Ensemble Learning Methods</u>\n","\n","1. Bagging\n","\n","2. Boosting\n","\n","**<u><h3>1. Bagging</h3></u>**\n","\n","Bagging(BootStrap Aggregation) idea is to create several sub sample from training sample. For each sub-sample is used to train decision tree. Average of all the predictions from different trees are used which is more robust than a single decision tree.\n","\n","<u>Steps:</u>\n","  1. Multiple subsets are created from the original dataset, selecting observations with replacement.\n","\n","  2. A base model (weak model) is created on each of these subsets.\n","\n","  3. The models run in parallel and are independent of each other.\n","\n","  4. The final predictions are determined by combining the predictions from all the models.\n","\n","\n","<center><img src = \"https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2019/01/Classification-Random-Forest-Random-Forest-In-R-Edureka-2-381x300.png\"></center>\n","\n","\n","**<u><h3>2. Boosting</h3></u>**\n","\n","Boosting is another ensemble technique to create a collection of predictors. In this technique, learners are learned sequentially. In other words, we fit consecutive trees (random sample) and at every step, the goal is to solve for net error from the prior tree.\n","\n","Letâ€™s understand the way boosting works in the below steps:\n","1. A subset is created from the original dataset,all data points are given equal weights.\n","2. A base model is created on this subset,then model is used to make predictions on the whole dataset.\n","3. Errors are calculated using the actual values and predicted values ,observations which are incorrectly predicted, are given higher weights.\n","4. Observations are selected which having higher weights to create the model.\n","5. Another model is created and predictions are made on the dataset.\n","6. Similarly, multiple models are created, each correcting the errors of the previous model.\n","7. The final model (strong learner) is the weighted mean of all the models (weak learners).\n","\n","<img src=\"https://littleml.files.wordpress.com/2017/03/boosted-trees-process.png\">\n","\n","\n","**<u>Bagging algorithms:</u>**\n","\n","1. Random forest\n","\n","**<u>Boosting algorithms:</u>**\n","\n","1. AdaBoost\n","2. GBM\n","3. XGBM\n","4. Light GBM\n","5. CatBoost"],"metadata":{"id":"eFjbvD44ULkP"}}]}