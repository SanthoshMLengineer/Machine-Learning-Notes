{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM7Guataf78LEByDOZ4ixkP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**<h1><center>NAIVE BAYES</center></h1>**\n","\n","Naive Bayes is a supervised learning algorithms used for classification. This algorithm is based on Bayes theorm. Assumption is that features are independent each other.\n","\n","<u>Bayes Theorem</u>\n","\n","Bayes Theorem finds the probability of an event occurring given the probability of another event that has already occurred.\n","\n","1. If Event \"A\" and \"B\" are not independent, then the probability of event \"A\" occuring given that event \"B\" occured is\n","\n","  ```P(A|B) = p(A intersection B) / P(B)```\n","\n","2. The probability of both happening is the chance of first one happens, and then second one given that first one happend.\n","  \n","  `p(A intersection B) = p(A) . p(B|A)`\n","\n","  therefore\n","\n","  ```\n","  P(A|B) = p(A) . p(B|A) / P(B)`\n","  \n","  p(A|B): Probability of event A given event B has already occurred\n","  \n","  p(B|A): Probability of event B given event A has already occuured\n","  \n","  p(A): Probability of event A\n","  \n","  p(B): Probability of event B\n","  ```\n","\n","3. Naive Bayes calculates the probability for the given input features is\n","\n","  ```\n","  p(yi|x1,x2,...xn) = p(yi) . p(x1, x2, x3, ..xn/ yi) /p(x1, x2,...xn)\n","  \n","  p(x1, x2 , … , xn | yi) is the probability of specific combination of feature given that label.\n","  p(yi | x1,x2, … , xn) here we need to find out probability of label given that input feature.\n","  ```\n","\n","<u> Their are three types of Naive Bayes Algorithm</u>\n","\n","1. Gaussian: It is used in classification and it assumes that features follow a normal distribution.\n","\n","2. Multinomial: It is used  for multiclass classification. observed over the n trials”.\n","\n","3. Bernoulli: It is used for binary classification.\n","\n","**<u>Application</u>**\n","\n","1. Text classification/ Spam Filtering/ Sentiment Analysis.\n","\n","2. Recommendation System.\n","\n","3. Multi-Class Classification.\n","\n","**<h4>Advantage and Disadvantage of Naive Bayes</h4>**\n","\n","<u>Advantage : </u> \n","\n","1. It is easy and fast to predict class of test data set. It also perform well in multi class prediction .\n","\n","2. It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n","\n","<u>Disadvanatage : </u>\n","\n","1. If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\n","\n","2. On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n","\n","\n","\n","\n"],"metadata":{"id":"ucvAOm7lyu9L"}}]}