{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**<h1><center>RANDOM FOREST</center></h1>**\n","\n","Random Forest is an extension over bagging. It takes one extra step where in addition to taking the random subset of data, it also takes the random selection of features rather than using all features to grow trees. When you have many random trees. It’s called Random Forest.\n","\n","Let’s look at the steps taken to implement Random forest:\n","1. Suppose there are N observations and M features in training data set. First, a sample from training data set is taken randomly with replacement.\n","2. A subset of M features are selected randomly and whichever feature gives the best split is used to split the node iteratively.\n","3. The tree is grown to the largest.\n","4. Above steps are repeated and prediction is given based on the aggregation of predictions from n number of trees.\n","\n","<u>Advantage : </u>\n","\n","1. Handles higher dimensionality data very well.\n","\n","2. Handles missing values and maintains accuracy for missing data.\n","\n","<u>Disadvantage : </u>\n","\n","Since final prediction is based on the mean predictions from subset trees, it won’t give precise values for the regression model.\n","\n","<center><img src = https://miro.medium.com/max/1400/0*PBGJw23ud8Sp7qO4. width = 500px></center>"],"metadata":{"id":"cbANWfxzaidb"}}]}