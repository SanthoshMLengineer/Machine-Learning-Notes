{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOUu9lip+hMybaFj44wNP/z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**<h1><center>XGBOOST</center></h1>**\n","\n","XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. XGBoost improves upon the base GBM framework through systems optimization and algorithmic enhancements.\n","\n","**<u>System Optimization:</u>**\n","\n","1. Parallelization\n","\n","2. Tree Pruning\n","\n","3. Hardware Optimization\n","\n","**<u>Algorithmic Enhancements</u>**\n","\n","1. Regularization\n","\n","2. Cross- Validation\n","\n","3. Sparsity Awareness\n","\n","\n","\n","**<u>Advantage</u>**\n","\n","\n","**Advantage**\n","\n","\n","1. <u>Parallel Processing:</u>\n","\n","  XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n","\n","2. <u>Handling Missing Values</u>\n","\n","  XGBoost has an in-built routine to handle missing values. The user is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.\n","\n","\n","3. <u>Tree Pruning:</u>\n","\n","  A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. \n","\n","\n","4. <u>Built-in Cross-Validation</u>\n","\n","  XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n"],"metadata":{"id":"_2BRUPtuUWaB"}}]}